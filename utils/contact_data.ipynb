{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bab6ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ef956f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unproject_pixels(pts, cam_matrix, vfov=55, near_plane=0.1, far_plane=100):\n",
    "    '''\n",
    "    pts: [N, 2] pixel coords\n",
    "    depth: [N, ] depth values\n",
    "    returns: [N, 3] world coords\n",
    "    '''\n",
    "\n",
    "    \n",
    "    camera_matrix = np.linalg.inv(cam_matrix.reshape((4, 4)))\n",
    "\n",
    "    # Different from real-world camera coordinate system.\n",
    "    # OpenGL uses negative z axis as the camera front direction.\n",
    "    # x axes are same, hence y axis is reversed as well.\n",
    "    # Source: https://learnopengl.com/Getting-started/Camera\n",
    "    rot = np.array([[1, 0, 0, 0],\n",
    "                    [0, -1, 0, 0],\n",
    "                    [0, 0, -1, 0],\n",
    "                    [0, 0, 0, 1]])\n",
    "    camera_matrix = np.dot(camera_matrix, rot)\n",
    "    # print(\"camera_matrix: \", camera_matrix)\n",
    "    # print(camera_matrix[1,:-1])\n",
    "\n",
    "    height = 256#512#depth_map.shape[0]\n",
    "    width = 256#512#depth_map.shape[1]\n",
    "\n",
    "    img_pixs = pts[:, [1, 0]].T\n",
    "    img_pix_ones = np.concatenate((img_pixs, np.ones((1, img_pixs.shape[1]))))\n",
    "\n",
    "    # Calculate the intrinsic matrix from vertical_fov.\n",
    "    # Motice that hfov and vfov are different if height != width\n",
    "    # We can also get the intrinsic matrix from opengl's perspective matrix.\n",
    "    # http://kgeorge.github.io/2014/03/08/calculating-opengl-perspective-matrix-from-opencv-intrinsic-matrix\n",
    "    vfov = vfov / 180.0 * np.pi\n",
    "    tan_half_vfov = np.tan(vfov / 2.0)\n",
    "    tan_half_hfov = tan_half_vfov * width / float(height)\n",
    "    fx = width / 2.0 / tan_half_hfov  # focal length in pixel space\n",
    "    fy = height / 2.0 / tan_half_vfov\n",
    "    intrinsics = np.array([[fx, 0, width/ 2.0],\n",
    "                           [0, fy, height / 2.0],\n",
    "                           [0, 0, 1]])\n",
    "    img_inv = np.linalg.inv(intrinsics[:3, :3])\n",
    "    cam_img_mat = np.dot(img_inv, img_pix_ones)\n",
    "\n",
    "    depth = -camera_matrix[1,-1]/np.dot(cam_img_mat.T, camera_matrix[1,:-1])\n",
    "    points_in_cam = np.multiply(cam_img_mat, depth)\n",
    "    points_in_cam = np.concatenate((points_in_cam, np.ones((1, points_in_cam.shape[1]))), axis=0)\n",
    "    points_in_world = np.dot(camera_matrix, points_in_cam)\n",
    "    points_in_world = points_in_world[:3, :].T#.reshape(3, height, width)\n",
    "    points_in_cam = points_in_cam[:3, :].T#.reshape(3, height, width)\n",
    "    \n",
    "    return points_in_world\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_square(center, num_points, rnge=1):\n",
    "    '''\n",
    "    center: [x, z, y]\n",
    "    '''\n",
    "    # Define the number of points along each side\n",
    "    \n",
    "    center_xy = center[[0, 2]]\n",
    "    \n",
    "    bnd = rnge/2\n",
    "\n",
    "    # Create a grid of x and y values between -1 and 1\n",
    "    x = np.linspace(-bnd, bnd, num=num_points)\n",
    "    y = np.linspace(-bnd, bnd, num=num_points)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "\n",
    "    # Create a mask for the points inside the square\n",
    "    mask = (X >= -1) & (X <= 1) & (Y >= -1) & (Y <= 1)\n",
    "\n",
    "    # Select the points inside the square\n",
    "    points = np.column_stack([X[mask], Y[mask]])\n",
    "    \n",
    "#     print(center_xy)\n",
    "#     \n",
    "    points = points + center_xy[None, :]\n",
    "    \n",
    "    points = pad_ones(points)\n",
    "    \n",
    "    points[:, 2] = center[1]\n",
    "    \n",
    "    points = points[:, [0, 2, 1]]\n",
    "    \n",
    "    return points\n",
    "\n",
    "def get_disc(center, num_points, rnge=1):\n",
    "    '''\n",
    "    center: [x, z, y]\n",
    "    '''\n",
    "    # Define the number of points along each side\n",
    "    \n",
    "    center_xy = center[[0, 2]]\n",
    "    \n",
    "#     bnd = rnge/2\n",
    "   \n",
    "    # Define the radius of the disc\n",
    "    radius = rnge\n",
    "\n",
    "    points = np.random.uniform(low=-2*radius, high=2*radius, size=(num_points, 2))\n",
    "\n",
    "    # Filter the points to keep only those within the disc\n",
    "    distances = np.sqrt(np.sum(points**2, axis=1))\n",
    "    disc_points = points[distances <= radius]\n",
    "    \n",
    "    disc_points = disc_points + center_xy[None, :]\n",
    "    \n",
    "    disc_points = pad_ones(disc_points)\n",
    "    \n",
    "    disc_points[:, 2] = center[1]\n",
    "    \n",
    "    disc_points = disc_points[:, [0, 2, 1]]\n",
    "    \n",
    "    return disc_points\n",
    "\n",
    "def filter_occluded(pts, im_seg, color):\n",
    "    '''\n",
    "    pts: [N, 2] pixel coords\n",
    "    returns: [K < N, 2] unoccluded coords\n",
    "    '''\n",
    "\n",
    "    target = (im_seg ==  color[None, None, :]).all(-1).astype('float')\n",
    "\n",
    "    im_seg = (im_seg >0).any(-1).astype('float')\n",
    "\n",
    "    im_seg = im_seg - target\n",
    "    \n",
    "    im_seg = im_seg > 0\n",
    "    \n",
    "    im_seg = ~im_seg\n",
    "    \n",
    "    valid = im_seg[pts[:, 0], pts[:, 1]]\n",
    "    \n",
    "    return pts[valid], im_seg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "594a811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = '/ccn2/u/rmvenkat/data/testing_physion/test_contacts_hw_speed/test_humans_consolidated'\n",
    "original_files = glob.glob(os.path.join(files, '**/**/**/*.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a11cc758",
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_map = '/ccn2/u/thekej/mcvd_physion_readout/test/test_map.json'\n",
    "with open(stim_map, 'r') as f:\n",
    "    stimulus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d189a4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_world_to_pixel(pw, cam_matrix, proj_matrix):\n",
    "    '''\n",
    "    pw: [N, 3] world coords\n",
    "    cam_matrix: [4, 4] cam matrix\n",
    "    proj_matrix: [4, 4] proj matrix\n",
    "    returns: [N, 2] pixel coords\n",
    "    '''\n",
    "    \n",
    "    matrix = np.matmul(proj_matrix, cam_matrix)\n",
    "    \n",
    "    pw = pad_ones(pw).T\n",
    "    \n",
    "    proj_pts = np.matmul(matrix, pw).T\n",
    "\n",
    "    proj_pts = proj_pts/proj_pts[:, 3:4]\n",
    "\n",
    "    proj_pts = proj_pts.clip(-1, 1)\n",
    "\n",
    "\n",
    "    proj_pts = (proj_pts + 1)/2\n",
    "    \n",
    "    proj_pts = proj_pts[:, :2]\n",
    "\n",
    "    proj_pts[:, 1] = 1 - proj_pts[:, 1]\n",
    "\n",
    "    proj_pts = proj_pts[:, [1, 0]]\n",
    "\n",
    "    proj_pts = (proj_pts*256).astype(int)\n",
    "    \n",
    "    return proj_pt\n",
    "\n",
    "def pad_ones(pts):\n",
    "    '''\n",
    "    pts: [N , K]\n",
    "    returns: [N, K+1]\n",
    "    '''\n",
    "    pw = np.concatenate([pts, np.ones([pts.shape[0], 1])], 1)\n",
    "    return pw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d52b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "\n",
    "def fill_masked_area(mask):\n",
    "    # Label the connected components in the image\n",
    "    labeled, num_features = ndimage.label(mask)\n",
    "    \n",
    "    # Compute the size of each connected component\n",
    "    component_sizes = ndimage.sum(mask, labeled, range(num_features + 1))\n",
    "    \n",
    "    # Identify the largest connected component\n",
    "    largest_component = component_sizes.argmax()\n",
    "    \n",
    "    # Create a binary mask where the largest component is set to 1 and everything else is 0\n",
    "    filled_mask = (labeled == largest_component).astype(int)\n",
    "    \n",
    "    return filled_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b83ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load hdf5\n",
    "filename = '/ccn2/u/thekej/R3M_readout/test_feats.hdf5'\n",
    "with h5py.File(filename, 'r') as f:\n",
    "    print(f.keys())\n",
    "    data = f['observed']\n",
    "    labels = f['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee09833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(f, f1, frame):\n",
    "    cam_matrix = f1['frames'][frame]['camera_matrices']['camera_matrix_cam0'][:]\n",
    "    cam_matrix = cam_matrix.reshape(4, 4)\n",
    "    proj_matrix = f1['frames'][frame]['camera_matrices']['projection_matrix_cam0'][:]\n",
    "    proj_matrix = proj_matrix.reshape(4, 4)\n",
    "\n",
    "    #print(contacts[()])\n",
    "    contacts = proj_world_to_pixel(np.array(f['frames'][frame]['collisions']['contacts_ot'][()]), cam_matrix, proj_matrix)\n",
    "    num_samp = 100000\n",
    "\n",
    "    pts = np.array([[contacts[0][0], contacts[0][1]]])\n",
    "    #print(pts)\n",
    "    pw = unproject_pixels(pts, cam_matrix)\n",
    "\n",
    "    pw_square = get_disc(pw[0], num_samp, rnge=0.2)\n",
    "\n",
    "    pixel_coord_recon_square = proj_world_to_pixel(pw_square, cam_matrix, proj_matrix)\n",
    "\n",
    "    mask = np.zeros((256, 256))\n",
    "\n",
    "    for pt in pixel_coord_recon_square:#_:\n",
    "        mask[pt[0]:pt[0]+1, pt[1]:pt[1]+1] = 1\n",
    "\n",
    "    filled = fill_masked_area(mask)\n",
    "    return filled, pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e77a1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimuli_names, stimuli_contacts = [], []\n",
    "counter = 0\n",
    "masks = []\n",
    "stimuli_indices = []\n",
    "for counter, o_f in enumerate(original_files): \n",
    "    if counter % 50 == 0:\n",
    "        print('Done %d out of %d'%(counter, len(original_files)))\n",
    "    file_name = o_f.split('/')[-3:]\n",
    "    base_path = ['/ccn2/u/rmvenkat/data/testing_physion/all_final_reproduce_monkey/test_consolidated/lf_0'] + file_name\n",
    "    file_path = '/'.join(base_path)\n",
    "    if not os.path.isfile(file_path):\n",
    "        continue\n",
    "\n",
    "    stimuli_name = '_'.join(file_path.split('/')[-2:]).replace('.hdf5', '')\n",
    "    contact_check = False\n",
    "    if not str(stimuli_name.encode()) in stimulus:\n",
    "        continue\n",
    "    with h5py.File(o_f, 'r') as f:\n",
    "        with h5py.File(file_path, 'r') as f1:\n",
    "            im_seg = np.array(Image.open(io.BytesIO(f1['frames']['0000']['images']['_id_cam0'][:])))\n",
    "            frames = list(f['frames'])\n",
    "            for i, frame in enumerate(frames):\n",
    "                contacts = f['frames'][frame]['collisions']['contacts_ot']\n",
    "                if contacts.shape[0] == 0:\n",
    "                    continue\n",
    "                mask, pts = get_mask(f, f1, frame)\n",
    "                contact_check = True\n",
    "                break\n",
    "            \n",
    "            if contact_check:\n",
    "                stimuli_contacts += [pts]\n",
    "                stimuli_names += [stimuli_name]\n",
    "                #stimuli_indices += [stimulus[str(stimuli_name.encode())]]\n",
    "                masks += [mask]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9483cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_map = '/ccn2/u/thekej/sgnn_features/test_objects_observed_full_outcome__map.json'\n",
    "with open(stim_map, 'r') as f:\n",
    "    stimulus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076bd59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = np.stack(masks)\n",
    "stimuli_contacts = np.stack(stimuli_contacts)\n",
    "\n",
    "stimuli_indices = []\n",
    "\n",
    "for name in stimuli_names:\n",
    "    stimuli_indices += [stimulus[str(name)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3bb474",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stim_map_file = '/ccn2/u/thekej/placement_task/sgnn_placement/test_observed_full_outcome_map.json'\n",
    "output_filename = '/ccn2/u/thekej/placement_task/sgnn_placement/test_feats_observed_full_outcome.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d36cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stimuli_map = {}\n",
    "for i, stim in enumerate(stimuli_names):\n",
    "    new_stimuli_map[stim] = i\n",
    "with open(new_stim_map_file, 'w') as f:\n",
    "    json.dump(new_stimuli_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aad9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load hdf5\n",
    "filename = '/ccn2/u/thekej/sgnn_features/test_objects_observed_full_outcome.hdf5'\n",
    "\n",
    "with h5py.File(filename, 'r') as f_in:\n",
    "    print(f_in.keys())\n",
    "    data = f_in['features'][:]\n",
    "    print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5651bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(output_filename, 'w') as f_out:\n",
    "    # Sort the indices and convert to numpy array\n",
    "    #sorted_indices = np.sort(indices)\n",
    "\n",
    "    # Load only the subset of data you need\n",
    "    subset = data[stimuli_indices]\n",
    "\n",
    "    # Save the subset to the new HDF5 file\n",
    "    f_out.create_dataset('features', data=subset)\n",
    "    f_out.create_dataset('original_indices', data=stimuli_indices)\n",
    "    f_out.create_dataset('contacts', data=stimuli_contacts)\n",
    "    f_out.create_dataset('masks', data=masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ba0cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(output_filename, 'w') as f_out:\n",
    "    # Sort the indices and convert to numpy array\n",
    "    #sorted_indices = np.sort(indices)\n",
    "\n",
    "    # Load only the subset of data you need\n",
    "    subset = data[stimuli_indices]\n",
    "    subset1 = data1[stimuli_indices]\n",
    "    subset2 = data2[stimuli_indices]\n",
    "\n",
    "    # Save the subset to the new HDF5 file\n",
    "    f_out.create_dataset('observed', data=subset)\n",
    "    f_out.create_dataset('observed_full_outcome', data=subset1)\n",
    "    f_out.create_dataset('simulation', data=subset1)\n",
    "    f_out.create_dataset('original_indices', data=stimuli_indices)\n",
    "    f_out.create_dataset('contacts', data=stimuli_contacts)\n",
    "    f_out.create_dataset('masks', data=masks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
