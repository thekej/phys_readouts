seed: 1
cache: false # caching available only for encoded datasets

# Training
batch_size: 8
num_workers: 4
lr: 0.0001
lr_schedule: "cosine"
weight_decay: 0.00001
total_steps: 1000000
warmup_steps: 5000
save_interval: 100000
viz_interval: 100000
log_interval: 100

# Data
data_path: "TODO"
eval_seq_len: 300
seq_len: 300
image_size: 128
channels: 3

num_partitions: 1
rng_keys: ["dropout", "sample"]
batch_keys: ["video", "actions"]

# Model
model: "teco"
vqvae_ckpt: "TODO"

encoder: # encoder / decoder are mirrored, with decoder depths reversed
  depths: [256, 512] # 16x16 -> 8x8
  blocks: 4

decoder: # encoder / decoder are mirrored, with decoder depths reversed
  depths: [256, 512] # 16x16 -> 8x8
  blocks: 6

z_ds: 8 # 8x8 -> 2x2
z_tfm_kwargs:
  embed_dim: 1024
  mlp_dim: 4096
  num_heads: 16
  num_layers: 12
  dropout: 0.
  attention_dropout: 0.

z_git:
  vocab_dim: 256
  mask_schedule: "cosine"
  tfm_kwargs:
    embed_dim: 768
    mlp_dim: 3072
    num_heads: 12
    num_layers: 6
    dropout: 0.
    attention_dropout: 0.

embedding_dim: 128
codebook:
  n_codes: 1024
  proj_dim: 32

n_cond: 1
drop_loss_rate: 0.9

# Actions
use_actions: true
action_dim: 6
action_embed_dim: 16
dropout_actions: false

# Sampling
T_draft: 8
T_revise: 8
M: 2
open_loop_ctx: 36
